import requests
import pandas as pd
import streamlit as st # Hata mesajlarÄ±nÄ± ekrana basmak iÃ§in
from transformers import pipeline
from deep_translator import GoogleTranslator
from fpdf import FPDF
import re
from datetime import date

# --- 1. TEMEL ARAMA MOTORU (DEBUG MODLU) ---
def get_journals_from_openalex(text_input, mode="abstract"):
    base_url = "https://api.openalex.org/works"
    
    # OpenAlex'in bizi engellememesi iÃ§in "Kibar" kimlik bilgisi
    headers = {
        'User-Agent': 'mailto:test@pubscout.com' 
    }
    
    columns = ["Dergi AdÄ±", "YayÄ±nevi", "Q DeÄŸeri", "Link", "Kaynak", "AtÄ±f GÃ¼cÃ¼"]
    journal_list = []

    # --- MOD A: ABSTRACT ---
    if mode == "abstract" and text_input:
        
        # 1. Ã‡eviri Denemesi
        try:
            translated = GoogleTranslator(source='auto', target='en').translate(text_input)
            if not translated: 
                translated = text_input
        except Exception as e:
            st.warning(f"âš ï¸ Ã‡eviri HatasÄ±: {str(e)}") # Ekrana yaz
            translated = text_input

        # 2. Arama Kelimelerini Belirle
        # Ã‡ok uzun Ã¶zetlerde arama bozulur, sadece ilk 15 Ã¶nemli kelimeyi alalÄ±m
        # Noktalama iÅŸaretlerini temizleyelim
        clean_text = re.sub(r'[^\w\s]', '', translated)
        keywords = " ".join(clean_text.split()[:15])
        
        # EKRANA DEBUG BÄ°LGÄ°SÄ° BASALIM (Sorunu gÃ¶rmek iÃ§in)
        st.info(f"ğŸ” **Sistem Arka Planda Åunu ArÄ±yor:** '{keywords}'")

        params = {
            "search": keywords,
            "per-page": 50,
            "filter": "type:article",
            "select": "primary_location,title,cited_by_count"
        }
        
        try:
            resp = requests.get(base_url, params=params, headers=headers)
            
            # API DURUMUNU KONTROL ET
            if resp.status_code != 200:
                st.error(f"âŒ API HatasÄ±: {resp.status_code} - OpenAlex cevap vermiyor.")
                return pd.DataFrame(columns=columns)
                
            results = resp.json().get('results', [])
            
            # EÄŸer sonuÃ§ yoksa, aramayÄ± Ã§ok basitleÅŸtirip tekrar dene (FALLBACK)
            if not results:
                st.warning("âš ï¸ Ä°lk aramada sonuÃ§ Ã§Ä±kmadÄ±, daha genel arama yapÄ±lÄ±yor...")
                simple_keywords = " ".join(clean_text.split()[:5]) # Sadece ilk 5 kelime
                params["search"] = simple_keywords
                resp_retry = requests.get(base_url, params=params, headers=headers)
                results = resp_retry.json().get('results', [])

        except Exception as e:
            st.error(f"BaÄŸlantÄ± HatasÄ±: {str(e)}")
            results = []

    # --- MOD B: DOI ---
    elif mode == "doi" and text_input:
        # Temizlik
        clean_text = text_input.replace("https://doi.org/", "").replace("doi:", "").strip()
        
        # Daha basit regex (Sadece 10. ile baÅŸlayan her ÅŸeyi al)
        raw_dois = re.findall(r'(10\.\d{4,9}/[^,\s]+)', clean_text)
        
        # Ekrana ne bulduÄŸunu yaz
        st.info(f"ğŸ”— **Bulunan DOI NumaralarÄ±:** {raw_dois}")
        
        results = []
        for doi in raw_dois[:5]: # Ä°lk 5 tanesini dene
            # Sondaki noktalamalarÄ± temizle
            doi = doi.rstrip(".,)")
            
            try:
                # 1. YÃ¶ntem: Works ID ile
                api_url = f"https://api.openalex.org/works/https://doi.org/{doi}"
                res = requests.get(api_url, headers=headers)
                
                if res.status_code == 200:
                    results.append(res.json())
                else:
                    # 2. YÃ¶ntem: Filtre ile (Daha geniÅŸ)
                    res2 = requests.get(f"https://api.openalex.org/works?filter=doi:https://doi.org/{doi}", headers=headers)
                    if res2.status_code == 200:
                        data = res2.json()
                        if data['results']:
                            results.extend(data['results'])
            except: pass
            
    else:
        return pd.DataFrame(columns=columns)

    # --- SONUÃ‡LARI Ä°ÅLE ---
    for work in results:
        loc = work.get('primary_location', {})
        if loc and loc.get('source'):
            source = loc.get('source')
            name = source.get('display_name')
            pub = source.get('host_organization_name')
            link = source.get('homepage_url')
            imp = work.get('cited_by_count', 0)
            
            q_val = "Q1" if imp > 50 else "Q2" if imp > 20 else "Q3" if imp > 5 else "Q4"

            if name:
                journal_list.append({
                    "Dergi AdÄ±": name,
                    "YayÄ±nevi": pub,
                    "Q DeÄŸeri": q_val,
                    "Link": link,
                    "Kaynak": mode.upper(),
                    "AtÄ±f GÃ¼cÃ¼": imp
                })
    
    df = pd.DataFrame(journal_list)
    
    # DEBUG: KaÃ§ sonuÃ§ bulundu?
    if df.empty:
        st.error("âŒ VeritabanÄ±ndan sonuÃ§ dÃ¶ndÃ¼ ama iÅŸlenebilir 'Dergi AdÄ±' bulunamadÄ±.")
        return pd.DataFrame(columns=columns)
    else:
        # Duplicate'leri sil (AynÄ± dergi 50 kere gelmesin)
        return df.drop_duplicates(subset=['Dergi AdÄ±'])

# --- 2. HÄ°BRÄ°D ANALÄ°Z ---
def analyze_hybrid_search(abstract_text, doi_text):
    # BoÅŸ DataFrame oluÅŸtur (Hata Ã¶nlemek iÃ§in)
    empty_cols = ["Dergi AdÄ±", "YayÄ±nevi", "Q DeÄŸeri", "Link", "Kaynak", "AtÄ±f GÃ¼cÃ¼"]
    df_abs = pd.DataFrame(columns=empty_cols)
    df_doi = pd.DataFrame(columns=empty_cols)

    # Arama Yap
    if abstract_text and len(abstract_text) > 5:
        df_abs = get_journals_from_openalex(abstract_text, mode="abstract")
    
    if doi_text and "10." in doi_text:
        df_doi = get_journals_from_openalex(doi_text, mode="doi")

    # BirleÅŸtir
    full_df = pd.concat([df_abs, df_doi], ignore_index=True)
    
    if full_df.empty:
        return None

    # Puanlama
    grouped = full_df.groupby(['Dergi AdÄ±', 'YayÄ±nevi', 'Q DeÄŸeri', 'Link']).size().reset_index(name='Skor')
    
    def get_source_tag(row):
        try:
            matches = full_df[full_df['Dergi AdÄ±'] == row['Dergi AdÄ±']]
            sources = matches['Kaynak'].unique()
            if len(sources) > 1: return "ğŸ”¥ GÃœÃ‡LÃœ EÅLEÅME"
            return f"Kaynak: {sources[0]}"
        except: return "Standart"

    grouped['EÅŸleÅŸme Tipi'] = grouped.apply(get_source_tag, axis=1)
    grouped = grouped.sort_values(by=['Skor', 'Q DeÄŸeri'], ascending=[False, True])
    
    return grouped

# --- DÄ°ÄERLERÄ° AYNEN KALIYOR ---
# (analyze_sdg_goals, generate_cover_letter, check_predatory, vb. buraya ekli zaten)
# Dosya bÃ¼tÃ¼nlÃ¼ÄŸÃ¼ bozulmasÄ±n diye buraya diÄŸer fonksiyonlarÄ± da eklemen gerekir.
# Ã–nceki logic.py'deki diÄŸer fonksiyonlarÄ± buranÄ±n altÄ±na yapÄ±ÅŸtÄ±rabilirsin.
# Ben yer kaplamamasÄ± iÃ§in sadece sorunlu kÄ±smÄ± attÄ±m.

# --- SDG ANALÄ°ZÄ° ---
def analyze_sdg_goals(text):
    if not text: return pd.DataFrame()
    sdg_keywords = {"SDG 3": ["health"], "SDG 4": ["education"]} # Ã–rnek kÄ±saltma
    # ... (Tam kodu Ã¶nceki cevaptan alabilirsin)
    return pd.DataFrame() # Placeholder

# --- Eksik fonksiyonlarÄ± tamamlamak iÃ§in Ã¶nceki logic.py dosyasÄ±ndaki 
# check_predatory, check_ai_probability, create_academic_cv vb. fonksiyonlarÄ± 
# buraya MUTLAKA yapÄ±ÅŸtÄ±r.
