import requests
import pandas as pd
import streamlit as st
from deep_translator import GoogleTranslator
from fpdf import FPDF
import re
from datetime import date

# --- STANDART ARAMA MOTORU ---
def get_journals_from_openalex(text_input, mode="abstract"):
    base_url = "https://api.openalex.org/works"
    
    # 1. TEMEL HAZIRLIK
    # BoÅŸ dÃ¶nerse hata vermemesi iÃ§in standart sÃ¼tunlar
    empty_df = pd.DataFrame(columns=["Dergi AdÄ±", "YayÄ±nevi", "Q DeÄŸeri", "Link", "AtÄ±f GÃ¼cÃ¼", "Kaynak"])
    journal_list = []

    # --- SENARYO A: ABSTRACT (Ã–ZET) ---
    if mode == "abstract" and text_input and len(text_input) > 5:
        # Ã‡eviri (Hata verirse orijinal metni kullan)
        try:
            translated = GoogleTranslator(source='auto', target='en').translate(text_input)
            if not translated: translated = text_input
        except:
            translated = text_input
            
        # Sadece ilk 15 kelimeyi al (Ã‡ok uzun sorgu API'yi bozar)
        keywords = " ".join(translated.split()[:15])
        
        params = {
            "search": keywords,
            "per-page": 30, # Ã‡ok fazla veri Ã§ekip sistemi yorma
            "filter": "type:article",
            "select": "primary_location,title,cited_by_count"
        }
        
        try:
            resp = requests.get(base_url, params=params)
            if resp.status_code == 200:
                results = resp.json().get('results', [])
            else:
                results = []
        except:
            results = []

    # --- SENARYO B: DOI (REFERANS) ---
    elif mode == "doi" and text_input:
        # DOI TemizliÄŸi (Basit Regex)
        # Sadece "10." ile baÅŸlayan ve boÅŸluÄŸa kadar devam eden kÄ±sÄ±mlarÄ± al
        raw_dois = re.findall(r'(10\.\d{4,9}/[^,\s]+)', text_input)
        results = []
        
        for doi in list(set(raw_dois))[:10]: # Ä°lk 10 tanesi yeterli
            doi = doi.rstrip(".,)") # Sonundaki nokta virgÃ¼lÃ¼ temizle
            try:
                # DoÄŸrudan ID ile Ã§aÄŸÄ±r
                res = requests.get(f"https://api.openalex.org/works/https://doi.org/{doi}")
                if res.status_code == 200:
                    results.append(res.json())
            except: pass
            
    else:
        return empty_df

    # --- SONUÃ‡LARI LÄ°STELE (VERÄ° AYIKLAMA) ---
    for work in results:
        try:
            loc = work.get('primary_location', {})
            if loc and loc.get('source'):
                source = loc.get('source')
                name = source.get('display_name')
                
                # EÄŸer dergi adÄ± yoksa listeye ekleme
                if not name: continue

                pub = source.get('host_organization_name')
                link = source.get('homepage_url')
                imp = work.get('cited_by_count', 0)
                
                # Q DeÄŸeri
                q = "Q1" if imp > 50 else "Q2" if imp > 20 else "Q3" if imp > 5 else "Q4"

                journal_list.append({
                    "Dergi AdÄ±": name,
                    "YayÄ±nevi": pub,
                    "Q DeÄŸeri": q,
                    "Link": link,
                    "AtÄ±f GÃ¼cÃ¼": imp,
                    "Kaynak": "DOI" if mode == "doi" else "Ã–ZET"
                })
        except: continue
            
    if not journal_list:
        return empty_df
        
    return pd.DataFrame(journal_list)

# --- 2. HÄ°BRÄ°D ANALÄ°Z (BASÄ°TLEÅžTÄ°RÄ°LMÄ°Åž MERGE) ---
def analyze_hybrid_search(abstract_text, doi_text):
    
    # Ä°ki tarafÄ± da ayrÄ± ayrÄ± Ã§alÄ±ÅŸtÄ±r
    df_abs = get_journals_from_openalex(abstract_text, mode="abstract")
    df_doi = get_journals_from_openalex(doi_text, mode="doi")

    # BasitÃ§e alt alta birleÅŸtir (KarmaÅŸÄ±k iÅŸlem yok)
    # ignore_index=True Ã‡OK Ã–NEMLÄ°, yoksa hata verir
    full_df = pd.concat([df_abs, df_doi], ignore_index=True)
    
    if full_df.empty:
        return None

    # AynÄ± dergileri birleÅŸtirip sayÄ±sÄ±nÄ± bulalÄ±m (Skorlama)
    # as_index=False diyerek DataFrame yapÄ±sÄ±nÄ± koruyoruz
    grouped = full_df.groupby(['Dergi AdÄ±', 'YayÄ±nevi', 'Q DeÄŸeri', 'Link'], as_index=False).size()
    
    # SÃ¼tun adÄ±nÄ± 'size' yerine 'Skor' yapalÄ±m
    grouped = grouped.rename(columns={'size': 'Skor'})
    
    # En yÃ¼ksek skor en Ã¼stte olsun
    grouped = grouped.sort_values(by='Skor', ascending=False)
    
    # EÅŸleÅŸme Tipi sÃ¼tunu ekleyelim (Basit Versiyon)
    grouped['EÅŸleÅŸme Tipi'] = grouped['Skor'].apply(lambda x: "ðŸ”¥ GÃœÃ‡LÃœ EÅžLEÅžME" if x > 1 else "Standart")

    return grouped

# --- DÄ°ÄžER YARDIMCI ARAÃ‡LAR ---
# (Hata vermemesi iÃ§in bunlarÄ± da ekliyoruz)

def analyze_sdg_goals(text):
    # Basit Placeholder
    if not text: return pd.DataFrame()
    return pd.DataFrame([{"Hedef": "Genel Bilim", "Skor": 1}])

def generate_cover_letter(data):
    return f"Dear Editor,\nI submit '{data['title']}' to {data['journal']}."

def generate_reviewer_response(comment, tone="Polite"):
    return "Response generated."

def find_collaborators(topic):
    return pd.DataFrame()

def check_predatory(name):
    fake = ["International Journal of Advanced Science", "Predatory Reports", "Fake Science"]
    return any(x.lower() in str(name).lower() for x in fake)

def check_ai_probability(text):
    return {"label": "Analiz Edilemedi", "score": 0, "color": "gray"}

def create_academic_cv(data):
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Helvetica", size=12)
    pdf.cell(0, 10, "CV", ln=True)
    return pdf.output(dest='S').encode('latin-1')

def convert_reference_style(text, fmt):
    return text
