import requests
import pandas as pd
import streamlit as st
from deep_translator import GoogleTranslator
from fpdf import FPDF
import re
from datetime import date

# --- YENÄ°: 1. TREND ANALÄ°ZÄ° MOTORU ğŸ“ˆ ---
def analyze_trends(topic):
    """
    Bir konunun son 10 yÄ±ldaki yÃ¼kseliÅŸ/dÃ¼ÅŸÃ¼ÅŸ trendini analiz eder.
    """
    base_url = "https://api.openalex.org/works"
    headers = {'User-Agent': 'mailto:admin@pubscout.com'}
    
    # Konuyu Ä°ngilizceye Ã§evir (Daha iyi sonuÃ§ iÃ§in)
    try:
        topic_en = GoogleTranslator(source='auto', target='en').translate(topic)
    except: topic_en = topic

    params = {
        "search": topic_en,
        "group_by": "publication_year",
        "per_page": 200
    }
    
    try:
        resp = requests.get(base_url, params=params, headers=headers)
        data = resp.json().get('group_by', [])
        
        # DataFrame'e Ã§evir ve son 10 yÄ±lÄ± filtrele
        df = pd.DataFrame(data)
        df = df[df['key'].astype(int) >= (date.today().year - 10)]
        df = df.sort_values('key') # YÄ±la gÃ¶re sÄ±rala
        df.columns = ['YÄ±l', 'Makale SayÄ±sÄ±']
        return df
    except:
        return pd.DataFrame()

# --- YENÄ°: 2. HÄ°BE VE FON BULUCU ğŸ’° ---
def find_funders(topic):
    """
    Bu konuyu en Ã§ok fonlayan kurumlarÄ± bulur.
    """
    base_url = "https://api.openalex.org/works"
    headers = {'User-Agent': 'mailto:admin@pubscout.com'}
    
    try:
        topic_en = GoogleTranslator(source='auto', target='en').translate(topic)
    except: topic_en = topic

    params = {
        "search": topic_en,
        "select": "grants",
        "per-page": 50 
    }
    
    try:
        resp = requests.get(base_url, params=params, headers=headers)
        results = resp.json().get('results', [])
        
        funder_list = []
        for work in results:
            for grant in work.get('grants', []):
                if grant.get('funder'):
                    funder_list.append(grant['funder'])
        
        if not funder_list: return pd.DataFrame()
        
        # SayÄ±m yap
        df = pd.DataFrame(funder_list).value_counts().reset_index()
        df.columns = ['Kurum AdÄ±', 'DesteklediÄŸi Makale SayÄ±sÄ±']
        return df.head(10) # Ä°lk 10 fon saÄŸlayÄ±cÄ±
    except:
        return pd.DataFrame()

# --- YENÄ°: 3. LÄ°TERATÃœR HARÄ°TASI (KAVRAMLAR) ğŸ§  ---
def analyze_concepts(topic):
    """
    Konuyla iliÅŸkili diÄŸer akademik kavramlarÄ± (Concepts) bulur.
    """
    base_url = "https://api.openalex.org/concepts"
    params = {"search": topic}
    
    try:
        resp = requests.get(base_url, params=params)
        results = resp.json().get('results', [])
        
        concepts = []
        for c in results:
            concepts.append({
                "Kavram": c['display_name'],
                "Seviye": c['level'], # 0: Genel, 1: Alt Dal
                "Alaka Skoru": c['relevance_score'],
                "Makale SayÄ±sÄ±": c['works_count']
            })
        return pd.DataFrame(concepts).head(10)
    except:
        return pd.DataFrame()

# --- MEVCUT FONKSÄ°YONLAR (AYNEN KORUNDU) ---
def get_journals_from_openalex(text_input, mode="abstract"):
    base_url = "https://api.openalex.org/works"
    headers = {'User-Agent': 'mailto:admin@pubscout.com'}
    columns = ["Dergi AdÄ±", "YayÄ±nevi", "Q DeÄŸeri", "Link", "Kaynak", "AtÄ±f GÃ¼cÃ¼"]
    journal_list = []

    if mode == "abstract" and text_input:
        try:
            translated = GoogleTranslator(source='auto', target='en').translate(text_input)
            if not translated: translated = text_input
        except: translated = text_input
        keywords = " ".join(translated.split()[:20])
        params = {"search": keywords, "per-page": 50, "filter": "type:article", "select": "primary_location,title,cited_by_count"}
        try:
            resp = requests.get(base_url, params=params, headers=headers)
            results = resp.json().get('results', [])
            if not results:
                short = " ".join(translated.split()[:6])
                resp = requests.get(base_url, params={"search":short, "per-page":50}, headers=headers)
                results = resp.json().get('results', [])
        except: results = []

    elif mode == "doi" and text_input:
        clean = text_input.replace("https://doi.org/", "").replace("doi:", "").strip()
        dois = list(set(re.findall(r'(10\.\d{4,9}/[-._;()/:a-zA-Z0-9]+)', clean)))
        results = []
        for d in dois[:10]:
            d = d.rstrip(".,)")
            try:
                r = requests.get(f"https://api.openalex.org/works/https://doi.org/{d}", headers=headers)
                if r.status_code == 200: results.append(r.json())
            except: pass
    else: return pd.DataFrame(columns=columns)

    for w in results:
        try:
            loc = w.get('primary_location', {})
            if loc and loc.get('source'):
                src = loc.get('source')
                nm = src.get('display_name')
                if not nm: continue
                imp = w.get('cited_by_count', 0)
                q = "Q1" if imp > 50 else "Q2" if imp > 20 else "Q3" if imp > 5 else "Q4"
                journal_list.append({"Dergi AdÄ±": nm, "YayÄ±nevi": src.get('host_organization_name'), "Q DeÄŸeri": q, "Link": src.get('homepage_url'), "Kaynak": mode.upper(), "AtÄ±f GÃ¼cÃ¼": imp})
        except: continue
    df = pd.DataFrame(journal_list)
    return df.drop_duplicates('Dergi AdÄ±') if not df.empty else pd.DataFrame(columns=columns)

def analyze_sdg_goals(text):
    if not text: return pd.DataFrame()
    keys = {"SDG 3 (SaÄŸlÄ±k)": ["health"], "SDG 4 (EÄŸitim)": ["education"], "SDG 9 (Teknoloji)": ["ai"], "SDG 13 (Ä°klim)": ["climate"]}
    txt = str(text).lower()
    m = [{"Hedef": k, "Skor": sum(1 for x in v if x in txt)} for k,v in keys.items()]
    return pd.DataFrame(m).sort_values("Skor", ascending=False)

def generate_cover_letter(data): return f"Dear Editor,\nSubmission: {data['title']}"
def generate_reviewer_response(c, t): return "Response generated."
def find_collaborators(topic):
    # Basit collaborator fonksiyonu
    return pd.DataFrame() # Åimdilik boÅŸ dÃ¶nsÃ¼n, trendlere odaklanalÄ±m
def check_predatory(name): return False
def check_ai_probability(text): return None
def create_academic_cv(data):
    pdf = FPDF(); pdf.add_page(); pdf.set_font("Helvetica", size=12); pdf.cell(40, 10, "CV"); return pdf.output(dest='S').encode('latin-1')
def convert_reference_style(text, fmt): return text
